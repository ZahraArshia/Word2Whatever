# Word2Whatever!
> An improvment of shallow word embeddings to challenge deep ones! deep learning is not always the solution!

![image](https://user-images.githubusercontent.com/78906545/187692286-022f40c9-bdcc-4bbd-97bf-6ef4a55885ef.png)

The need to convert human understandable text data into mathematically processable data has been the subject of researchers' studies even before the emergence of concepts such as artificial intelligence and natural language processing. In recent years, this concept has been more widely used and better-known due to the increase in the use of tasks related to natural language processing. However, since natural language processing tasks strongly depend on word embeddings, efforts are always made to provide a better and more efficient word embedding approach.

In this project, we present a method to improve the performance of word embedding. Our proposed method can address some of the problems of the current embedding methods. The provided approach is based on the existing statistical methods by changing the data structure of classical word embeddings. It can act as an improvement on previous embeddings or independently as a method for producing new embeddings.

## Getting Started

## Author

**Zahra Aershia**

- GitHub: [@ZahraArshia](https://github.com/ZahraArshia)
- Twitter: [@ZahraArshia](https://twitter.com/ZahraArshia)
- LinkedIn: [@ZahraArshia](https://www.linkedin.com/in/ZahraArshia/)


## Contributing

Contributions, issues, and feature requests are welcome!

Feel free to check the [issues page](https://github.com/ZahraArshia/Word2Whatever/issues).

## Show your support

Give a ⭐️ if you like this project!

## Acknowledgments

We would like to acknowledge [MUT NLP lab](https://github.com/mutnlp) for their support.

## License

This project is [MIT](https://github.com/ZahraArshia/Word2Whatever/blob/main/LICENSE) licensed.
